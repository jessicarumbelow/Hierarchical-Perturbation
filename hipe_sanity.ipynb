{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import scipy.misc\n",
    "import scipy.ndimage\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import ast\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "import time\n",
    "import glob\n",
    "with open('data/imagenet_classes.txt') as f:\n",
    "    classes = [line.split(' ')[-1].strip() for line in f.readlines()]\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from skimage.transform import resize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def old_occ(input, model, c, k_size=1, vis=False, step_size=1, c_img=None):\n",
    "\n",
    "    fig, axs = plt.subplots()\n",
    "    input_min = torch.mean(input[0], dim =(-1,-2), keepdim=True)\n",
    "    _, channels, yd, xd = input.shape\n",
    "    unocc_pred = model(input)\n",
    "    x_steps = range(0, xd - k_size + 1, step_size)\n",
    "    y_steps = range(0, yd - k_size + 1, step_size)\n",
    "    heatmap = np.zeros((len(y_steps), len(x_steps)))\n",
    "    num_occs = 0\n",
    "    hx = 0\n",
    "    for x in x_steps:\n",
    "        hy = 0\n",
    "        for y in y_steps:\n",
    "            occ_im = torch.Tensor(input.numpy().copy())\n",
    "            occ_im[:,:,y: y + k_size, x: x + k_size] = torch.mean(occ_im[:,:,y: y + k_size, x: x + k_size][0], dim =(-1,-2), keepdim=True)\n",
    "            diff = torch.relu(unocc_pred - model(occ_im))[0][c]\n",
    "            heatmap[hy, hx] = diff\n",
    "            num_occs += 1\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            if vis:\n",
    "                plt.show() \n",
    "                oi = np.transpose(original_input[:,:,:input_y_dim, :input_x_dim][0], (1, 2, 0))\n",
    "                plt.imshow(oi.reshape(28,28), cmap='gray')              \n",
    "                plt.imshow(clean_smap(heatmap, c_img.shape), alpha=0.5, cmap=plt.cm.jet)\n",
    "                print(x + y, '/', len(y_steps)*len(x_steps))\n",
    "            \n",
    "            print(num_occs, diff)\n",
    "            \n",
    "            hy += 1\n",
    "        hx += 1\n",
    "    \n",
    "    return clean_smap(heatmap, c_img.shape), num_occs\n",
    "\n",
    "\n",
    "def rise(inp, model, c, num_occs, vis=False, c_img=None):\n",
    "    fig, axs = plt.subplots()\n",
    "    \n",
    "    pred = model(inp)[0][c]\n",
    "    N = num_occs\n",
    "    s = 8\n",
    "    p1 = 0.5\n",
    "    inp_size = np.array(inp.shape[2:])\n",
    "    cell_size = np.ceil(inp_size / s)\n",
    "    up_size = (s + 1) * cell_size\n",
    "    grid = np.random.rand(N, s, s) < p1\n",
    "    grid = grid.astype('float32')\n",
    "    \n",
    "    masks = []\n",
    "    preds = []\n",
    "    sals = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        x = np.random.randint(0, cell_size[0])\n",
    "        y = np.random.randint(0, cell_size[1])\n",
    "        \n",
    "        mask = resize(grid[i], up_size, order=1, mode='reflect', anti_aliasing=False)[x:x + inp_size[0], y:y + inp_size[1]]\n",
    "        masks.append(mask)\n",
    "    \n",
    "        pred = model(torch.Tensor(masks[i]) * inp)[0][c].detach().numpy()\n",
    "        preds.append(pred)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if vis:\n",
    "            plt.show() \n",
    "            oi = np.transpose(inp[0], (1, 2, 0))\n",
    "            plt.imshow(oi.reshape(28,28), cmap='gray')\n",
    "            sal = np.asarray(preds).reshape(i+1, -1).T.dot(np.asarray(masks).reshape(i+1, -1)).reshape(*inp_size)\n",
    "            plt.imshow(clean_smap(sal, c_img.shape), alpha=0.5, cmap=plt.cm.jet)\n",
    "            print(pred)\n",
    "\n",
    "        print(i, '/', N)\n",
    "\n",
    "    heatmap = np.asarray(preds).reshape(N, -1).T.dot(np.asarray(masks).reshape(N, -1)).reshape(*inp_size)\n",
    "    heatmap = heatmap / N / p1\n",
    "    \n",
    "    return clean_smap(heatmap, c_img.shape), N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mnist_bin_occ(original_input, model, c, vis=False, min_k=1, c_img=None):\n",
    "    \n",
    "    input_min = 0.0#torch.mean(original_input[0], dim =(-1,-2), keepdim=True)\n",
    "    unoccluded_output = model(original_input).detach().numpy()#[0][c]\n",
    "    \n",
    "    _, channels, input_y_dim, input_x_dim = original_input.shape\n",
    "    y_ix, x_ix = 0, 0\n",
    "    y_dim = 2 ** int(np.ceil(np.log2(input_y_dim)))\n",
    "    x_dim = 2 ** int(np.ceil(np.log2(input_x_dim)))\n",
    "    input = np.zeros((_, channels, y_dim, x_dim))\n",
    "    input[:,:,:input_y_dim, :input_x_dim] = original_input\n",
    "    saliency_map = np.zeros((y_dim, x_dim))\n",
    "    num_occs = 0\n",
    "    depth = 0\n",
    "    branches = [(x_dim, x_ix, y_dim, y_ix, depth, 0)]\n",
    "    max_depth = (2 * max(np.log2(y_dim), np.log2(x_dim))) + 1\n",
    "    \n",
    "    fig, axs = plt.subplots()\n",
    "    \n",
    "    while len(branches) > 0:\n",
    "        x_dim, x_ix, y_dim, y_ix, depth, threshold = branches.pop()\n",
    "        \n",
    "        if x_ix < input_x_dim and y_ix < input_y_dim:\n",
    "            occluded_input = input.copy()\n",
    "            m = np.mean(occluded_input[:,:,y_ix:y_ix + y_dim - max(0, y_ix + y_dim - input_y_dim), x_ix:x_ix + x_dim - max(0, x_ix + x_dim - input_x_dim)][0], axis=(-1,-2), keepdims=True)\n",
    "            occluded_input[:,:,y_ix:y_ix + y_dim, x_ix:x_ix + x_dim] = input_min           \n",
    "            occluded_output = model(torch.Tensor(occluded_input[:,:,:input_y_dim, :input_x_dim])).detach().numpy()#[0][c]\n",
    "            output_difference = np.abs(np.sum(unoccluded_output - occluded_output))\n",
    "            num_occs+=1\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            \n",
    "            if output_difference > threshold:\n",
    "                depth += 1\n",
    "                threshold = np.mean(saliency_map) / (max_depth - depth)\n",
    "\n",
    "                saliency_map[y_ix:y_ix + y_dim, x_ix:x_ix + x_dim] += output_difference / ((y_dim - max(0, y_ix + y_dim - input_y_dim)) * (x_dim - max(0, x_ix + x_dim - input_x_dim)))\n",
    "                              \n",
    "                              \n",
    "                if vis:\n",
    "                    plt.show()\n",
    "                    oi = np.transpose(original_input[:,:,:input_y_dim, :input_x_dim][0], (1, 2, 0))\n",
    "                    plt.imshow(oi.reshape(28,28), cmap='gray')\n",
    "                    smap = saliency_map[:input_y_dim, :input_y_dim]\n",
    "                    plt.imshow(smap, alpha=0.5, cmap=plt.cm.jet)\n",
    "                    print(x_dim, y_dim)\n",
    "                     \n",
    "                print(num_occs, threshold, output_difference)\n",
    "                \n",
    "                next_x_ix, next_y_ix = x_ix, y_ix\n",
    "                if x_dim > y_dim:\n",
    "                    x_dim = x_dim // 2\n",
    "                    next_x_ix = x_ix + x_dim\n",
    "                else:\n",
    "                    y_dim = y_dim // 2\n",
    "                    next_y_ix = y_ix + y_dim\n",
    "                if (x_dim >= min_k) and (y_dim >= min_k):\n",
    "                    branches.extend([(x_dim, x_ix, y_dim, y_ix, depth, threshold), (x_dim, next_x_ix, y_dim, next_y_ix, depth, threshold)])\n",
    "    \n",
    "    heatmap = clean_smap(saliency_map[:input_y_dim, :input_y_dim], c_img.shape, sigma=8)\n",
    "    return heatmap, num_occs\n",
    "\n",
    "\n",
    "def mean_bin_occ(original_input, model, c, vis=False, min_k=2, c_img=None):\n",
    "    \n",
    "    input_min = torch.mean(original_input[0], dim =(-1,-2), keepdim=True)\n",
    "    unoccluded_output = model(original_input).detach().numpy()[0][c]\n",
    "    \n",
    "    _, channels, input_y_dim, input_x_dim = original_input.shape\n",
    "    y_ix, x_ix = 0, 0\n",
    "    y_dim = 2 ** int(np.ceil(np.log2(input_y_dim)))\n",
    "    x_dim = 2 ** int(np.ceil(np.log2(input_x_dim)))\n",
    "    input = np.zeros((_, channels, y_dim, x_dim))\n",
    "    input[:,:,:input_y_dim, :input_x_dim] = original_input\n",
    "    saliency_map = np.zeros((y_dim, x_dim))\n",
    "    num_occs = 0\n",
    "    depth = 0\n",
    "    branches = [(x_dim, x_ix, y_dim, y_ix, depth, 0)]\n",
    "    max_depth = (2 * max(np.log2(y_dim), np.log2(x_dim))) + 1\n",
    "    \n",
    "    fig, axs = plt.subplots()\n",
    "    \n",
    "    while len(branches) > 0:\n",
    "        x_dim, x_ix, y_dim, y_ix, depth, threshold = branches.pop()\n",
    "        \n",
    "        if x_ix < input_x_dim and y_ix < input_y_dim:\n",
    "            occluded_input = input.copy()\n",
    "            m = np.mean(occluded_input[:,:,y_ix:y_ix + y_dim - max(0, y_ix + y_dim - input_y_dim), x_ix:x_ix + x_dim - max(0, x_ix + x_dim - input_x_dim)][0], axis=(-1,-2), keepdims=True)\n",
    "            occluded_input[:,:,y_ix:y_ix + y_dim, x_ix:x_ix + x_dim] = m            \n",
    "            occluded_output = model(torch.Tensor(occluded_input[:,:,:input_y_dim, :input_x_dim])).detach().numpy()[0][c]\n",
    "            output_difference = np.max(unoccluded_output - occluded_output, 0)\n",
    "            num_occs+=1\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            \n",
    "            if output_difference > threshold:\n",
    "                depth += 1\n",
    "                threshold = np.mean(saliency_map) / (max_depth - depth)\n",
    "\n",
    "                saliency_map[y_ix:y_ix + y_dim, x_ix:x_ix + x_dim] += output_difference * depth\n",
    "                              \n",
    "                if vis:\n",
    "                    plt.show()\n",
    "                    oi = np.transpose(original_input[:,:,:input_y_dim, :input_x_dim][0], (1, 2, 0))\n",
    "                    plt.imshow(oi.reshape(28,28), cmap='gray')\n",
    "                    smap = saliency_map[:input_y_dim, :input_y_dim]\n",
    "                    plt.imshow(smap, alpha=0.5, cmap=plt.cm.jet)\n",
    "                    print(x_dim, y_dim)\n",
    "                     \n",
    "                print(num_occs, threshold, output_difference)\n",
    "                \n",
    "                next_x_ix, next_y_ix = x_ix, y_ix\n",
    "                if x_dim > y_dim:\n",
    "                    x_dim = x_dim // 2\n",
    "                    next_x_ix = x_ix + x_dim\n",
    "                else:\n",
    "                    y_dim = y_dim // 2\n",
    "                    next_y_ix = y_ix + y_dim\n",
    "                if (x_dim >= min_k) and (y_dim >= min_k):\n",
    "                    branches.extend([(x_dim, x_ix, y_dim, y_ix, depth, threshold), (x_dim, next_x_ix, y_dim, next_y_ix, depth, threshold)])\n",
    "    \n",
    "    heatmap = clean_smap(saliency_map[:input_y_dim, :input_y_dim], c_img.shape, sigma=8)\n",
    "    return heatmap, num_occs\n",
    "\n",
    "    \n",
    "def clean_smap(smap, img_shape, interp=PIL.Image.LANCZOS, sigma=8):\n",
    "    smap = Image.fromarray(smap-smap.min()).resize(img_shape[:-1], resample=interp)\n",
    "    #smap = gaussian_filter(smap, sigma=sigma)\n",
    "    return smap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on test set: 98 %\n",
      "Accuracy of     0 : 99 %\n",
      "Accuracy of     1 : 99 %\n",
      "Accuracy of     2 : 98 %\n",
      "Accuracy of     3 : 98 %\n",
      "Accuracy of     4 : 99 %\n",
      "Accuracy of     5 : 99 %\n",
      "Accuracy of     6 : 93 %\n",
      "Accuracy of     7 : 97 %\n",
      "Accuracy of     8 : 97 %\n",
      "Accuracy of     9 : 97 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_net = False\n",
    "batch_size = 4\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "classes = [str(n) for n in range(10)]\n",
    "\n",
    "PATH = './mnist_net.pth'\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.gradients = None\n",
    "\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.nn.functional.softmax(x, dim=1)\n",
    "\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "\n",
    "    def get_activations(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        return x\n",
    " \n",
    "\n",
    "\n",
    "def run_epochs(net, dataloader, criterion, optimizer, num_epochs, threshold):\n",
    "    \n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                avg_loss = running_loss / 2000\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, avg_loss))\n",
    "                if avg_loss <= threshold:\n",
    "                    return net\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    return net\n",
    "\n",
    "\n",
    "net = Net()\n",
    "num_epochs = 2\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "threshold = 0.05\n",
    "    \n",
    "if train_net:\n",
    "    \n",
    "    net = run_epochs(net, trainloader, criterion, optimizer, num_epochs, threshold)\n",
    "\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "else: \n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "net.eval()\n",
    "\n",
    "for data in testloader:\n",
    "    test_images, test_labels = data\n",
    "    outputs = net(test_images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += test_labels.size(0)\n",
    "    correct += (predicted == test_labels).sum().item()\n",
    "    \n",
    "    c = (predicted == test_labels).squeeze()\n",
    "    label = test_labels.item()\n",
    "    class_correct[label] += c.item()\n",
    "    class_total[label] += 1\n",
    "\n",
    "print('Accuracy of the network on test set: %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed Method, 15 Occlusions\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_hm(c_img, smap, title, im_name, predicted):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(c_img)\n",
    "    ax.imshow(smap, cmap=plt.cm.jet, alpha=0.5)\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "    plt.savefig('heatmaps/{}_pred_{}_{}'.format(im_name, predicted, title), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "net.eval()\n",
    "\n",
    "num_examples = 1\n",
    "save = True\n",
    "note = 'mnist'\n",
    "a = 1\n",
    "\n",
    "cmap= plt.cm.jet\n",
    "\n",
    "dataiter = iter(torch.utils.data.DataLoader(trainset, batch_size=1,shuffle=False, num_workers=2))\n",
    "ks = [1,2,6]\n",
    "num_cols = len(ks) + 5\n",
    "good_preds = 0\n",
    "n = 0\n",
    "cols = []\n",
    "smaps = []\n",
    "while n < num_examples:\n",
    "    n+=1\n",
    "    ti, tl = dataiter.next()\n",
    "\n",
    "    outputs = net(ti)[0]\n",
    "    predicted = classes[torch.argmax(outputs).item()]\n",
    "    actual = classes[tl]\n",
    "    p_class = torch.argmax(outputs)\n",
    "    confidence = max(outputs).item()\n",
    "\n",
    "    c_img = ti[0]\n",
    "    oi = np.transpose(c_img, (1, 2, 0)).reshape(28,28)\n",
    "    v = True\n",
    "\n",
    "    smap, occs = mnist_bin_occ(ti, net, p_class, vis=v, c_img=c_img)\n",
    "    smaps.append(smap)\n",
    "    c = 'Proposed Method, {} Occlusions'.format(int(occs))\n",
    "    print(c)\n",
    "    cols.append(c)\n",
    "    \"\"\"\n",
    "    smap, occs = rise(ti, net, p_class, occs, c_img=c_img, vis=v)\n",
    "    smaps.append(smap)\n",
    "    c = 'RISE {}, Occlusions'.format(int(occs))\n",
    "    print(c)\n",
    "    cols.append(c)\n",
    "\n",
    "    occs = 2000\n",
    "    smap, occs = rise(ti, net, p_class, occs, c_img=c_img, vis=v)\n",
    "    smaps.append(smap)\n",
    "    c = 'RISE {}, Occlusions'.format(int(occs))\n",
    "    print(c)\n",
    "    cols.append(c)\n",
    "\n",
    "    # Standard Occlusion\n",
    "    ks = [32, 64]\n",
    "    step = 8\n",
    "\n",
    "    for k in ks:\n",
    "        print('Standard Occlusion, kernel size {}...'.format(k))\n",
    "        smap, occs = old_occ(ti, net, p_class, k, vis=v, step_size=step, c_img=c_img)\n",
    "        smaps.append(smap)\n",
    "        c = 'Sliding Window K{} S{}, {} Occlusions'.format(k, step, int(occs))\n",
    "        print(c)\n",
    "        cols.append(c)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    thres = [0.01, 0.5]\n",
    "    \n",
    "    for t in thres:\n",
    "        smap, occs = thresh_bin_occ(ti, net, p_class, vis=v, c_img=c_img, threshold=t)\n",
    "        smaps.append(smap)\n",
    "        c = 'Proposed Method Threshold {} {} Occlusions'.format(int(t*10), int(occs))\n",
    "        print(c)\n",
    "        cols.append(c)\n",
    "        bin_occs.append(occs)\n",
    "        save_hm(c_img, smap, c, im_name.split('/')[-1].split('.')[0], predicted)\n",
    "    \n",
    "    smap, occs = quick_bin_occ(ti, net, p_class, vis=v, c_img=c_img)\n",
    "    smaps.append(smap)\n",
    "    c = 'Proposed Method Quick Version {} Occlusions'.format(int(occs))\n",
    "    print(c)\n",
    "    cols.append(c)\n",
    "    bin_occs.append(occs)\n",
    "    \n",
    "    save_hm(c_img, smap, c, im_name.split('/')[-1].split('.')[0], predicted)\n",
    "    \n",
    "    smap, occs = pix_bin_occ(ti, net, p_class, vis=v, c_img=c_img)\n",
    "    smaps.append(smap)\n",
    "    c = 'Proposed Method Pix Version {} Occlusions'.format(int(occs))\n",
    "    print(c)\n",
    "    cols.append(c)\n",
    "    bin_occs.append(occs)\n",
    "    \n",
    "    save_hm(c_img, smap, c, im_name.split('/')[-1].split('.')[0], predicted)\n",
    "    \n",
    "    smap, occs = bin_occ(ti, net, p_class, vis=v, c_img=c_img)\n",
    "    smaps.append(smap)\n",
    "    c = 'Proposed Method Auto Threshold {} Occlusions'.format(int(occs))\n",
    "    print(c)\n",
    "    cols.append(c)\n",
    "    bin_occs.append(occs)\n",
    "    \n",
    "    save_hm(c_img, smap, c, im_name.split('/')[-1].split('.')[0], predicted)\n",
    "\n",
    "    smap, occs = conf_bin_occ(ti, net, p_class, vis=v, c_img=c_img, threshold=confidence)\n",
    "    smaps.append(smap)\n",
    "    c = 'Proposed Method Confidence-Based Threshold {} {} Occlusions'.format(int(confidence*100), int(occs))\n",
    "    print(c)\n",
    "    cols.append(c)\n",
    "    bin_occs.append(occs)  \n",
    "    \n",
    "    save_hm(c_img, smap, c, im_name.split('/')[-1].split('.')[0], predicted)\n",
    "    \n",
    "    r_occs = max(bin_occs)\n",
    "    smap, occs = rise(ti, net, p_class, r_occs, c_img=c_img, vis=v)\n",
    "    smaps.append(smap)\n",
    "    c = 'RISE {} Occlusions'.format(int(r_occs))\n",
    "    print(c)\n",
    "    cols.append(c)\n",
    "    \n",
    "    save_hm(c_img, smap, c, im_name.split('/')[-1].split('.')[0], predicted)\n",
    "\n",
    "    # Standard Occlusion\n",
    "    ks = [64]\n",
    "    step = 8\n",
    "    \n",
    "    for k in ks:\n",
    "        print('Standard Occlusion, kernel size {}...'.format(k))\n",
    "        smap, occs = old_occ(ti, net, p_class, k, vis=v, step_size=step, c_img=c_img)\n",
    "        smaps.append(smap)\n",
    "        c = 'Sliding Window K{} S{} {} Occlusions'.format(k, step, int(occs))\n",
    "        print(c)\n",
    "        cols.append(c)\n",
    "        save_hm(c_img, smap, c, im_name.split('/')[-1].split('.')[0], predicted)\n",
    "    \n",
    "    num_cols = len(cols)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, num_cols, figsize=(10*num_cols, 10*num_cols))\n",
    "\n",
    "    for i in range(num_cols):\n",
    "        axs[i].imshow(c_img)\n",
    "        axs[i].imshow(smaps[i], cmap=plt.cm.jet, alpha=0.5)\n",
    "        axs[i].set_title(cols[i])\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.savefig('heatmaps/{}_{}'.format(im_name.split('/')[-1].split('.')[0], predicted), bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    img.close()\n",
    "\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
